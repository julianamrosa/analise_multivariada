---
title: "Lista 2 - Análise Multivariada"
author: "Juliana Magalhães Rosa"
date: "16/11/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Questão 1

__Marque Verdadeiro (V) ou Falso (F) para as seguintes afirmações.__

__1)*Curse of Dimensionality* é um problema encontrado em estudos com número muito grande de observações.__

Falso, é número de "variáveis" ou "parâmetos".

__2)Se $A=B^TB$ com $rank(B_{nXp})=p<n$, então $A$ é positiva definida.__

Verdadeiro, como $B$ é de posto completo, seu produto por sua transposta forma uma matriz positiva definida.

__3) Qualquer matriz $A$ pode ser fatorada em $T^TT$, sendo $T$ uma matriz triangular superior e não singular.__

Falso, $A$ deve ser quadrada e positiva definida para que a Decomposição de Cholesky possa ser aplicada.

__4) Se uma matriz quadrada $P$ é ortogonal, então $P^{−1} = P^T$.__

Verdadeiro, essa é a definição de uma matriz ortogonal.

__5) Uma matriz quadrada $A$ tem autovalor $\lambda > 0$ e autovetor correspondente $x \neq 0$, se $Ax = \lambda x$.__

Verdadeiro, essas são as definições de autovalores e autovetores.

__6) Seja um conjunto de dados com $n$ observações e $p$ variáveis. O traço da matriz de correlação destes dados é sempre igual a $p$.__

Verdadeiro, pois a matriz de correlações tem dimensões $pXp$ e sempre possui diagonal unitária (representando os coeficientes de correlação entre uma variável e ela mesma). Logo, a soma de $p$ elementos unitários é igual a $p$.

__7) Seja um conjunto de dados com $n$ observações e $p$ variáveis. O traço da matriz de variância-covariância ($\Sigma$) será igual à variância total ($TSV$) somente quando as variáveis forem independentes.__

Falso, a $TSV$ nada mais é do que a soma das variâncias das variáveis explicativas. Como a diagonal da matriz de covariâncias é justamente composta pelas variâncias de cada uma dessas variáveis, o taço da matriz será igual a essa soma de variâncias.

__8) Se $\lambda$ é autovalor de $A$ e $x$ é o autovetor correspondente, então $1 + \lambda$ é autovalor de $I + A$.__

Falso.

Se $\lambda$ é autovalor de $A$, então temos:

$$|A-\lambda I| =0$$

Para achar o autovalor $\lambda_B$ de $B=I+A$, devemos fazer:

$$|B-\lambda_B I| =0$$
$$|(I+A)-\lambda_B I| =0$$

$$|A-I-\lambda_B I| =0$$

$$|A-(1-\lambda_B) I| =0$$

Daí, observando a primeira equação, é possível perceber que:

$$(1-\lambda_B)=\lambda$$
Logo:

$$\lambda_B=1-\lambda$$

__9) A variância amostral total ($TSV$) ignora a estrutura de covariância das variáveis em estudo.__

Verdadeiro, é uma medida que leva em consideração apenas as variâncias de cada variável (em particular, a soma delas), não suas covariâncias entre si.

__10) A variância amostral generalizada ($GSV$) tem valores diferentes para diferentes padrões de variabilidade e associação e por isso serve para comparar variáveis no espaço p-dimensional.__

Falso, a $GSV$ depende tanto das variâncias como das covariâncias, então pode acontecer de diferentes matrizes de variância-covariância resultarem em um mesmo valor de $GSV$ (mesmo determinante para a matriz $S$). O uso da $GSV_R$, baseada na matriz de correlações, garante uma medida que depende apenas da associação entre as variáveis, pois neutraliza o efeito da variância. Portanto, a $GSV_R$ é mais adequada para comparações entre variáveis.

# Questão 2

__Provar o seguinte teorema:__

__Seja $X_{(nxk)}$ tal que $rank(X)=k<n$. Então, $P_X=X(X^TX)^{-1}X^T$ é idempotente e simétrica e consequenemente, uma matriz projeção ortogonal.__

DÚVIDA.

# Questão 3

__Utilizando o R verifique, através de exemplos, que uma matriz de projeção tem autovalores somente no conjunto ${0, 1}$. A demonstração pode ser feita utilizando a equação característica e lembrando que se $M$ é uma matriz de projeção, então $M = M^2 = M^T$.__

DÚVIDA.

# Questão 4

__Provar o seguinte teorema: Sejam $A$ e $B$ matrizes idempotentes. Então,__

__(a) $A + B$ é idempotente somente quando $AB = BA = 0$.__

$A+B$ ser idempotente significaria que:

$$A+B=(A+B)^2$$
$$A+B=(A+B)(A+B)$$

$$A+B=A^2+AB+BA+B^2$$

Lembrando que $A$ e $B$ são idempotentes, ou seja, $A=A^2$ e $B=B^2$, temos:

$$A+B=A+AB+BA+B$$
$$A-A+B-B=AB+BA$$
$$0=AB+BA$$
$$AB=-BA$$

E isso só se satisfaz se $AB=BA=0$.


__(b) $C = AB$ é idempotente somente quando $AB = BA$.__

DÚVIDA.

__(c) $I − A$ é idempotente.__

$$(I-A)^2=(I-A)(I-A)$$
$$(I-A)^2=I^2-IA-AI+A^2$$

$$(I-A)^2=I^2-A-A+A^2$$

$$(I-A)^2=I^2-2A+A^2$$

Como $A$ é idempotente e $I$ também, temos que $A=A^2$ e $I=I^2$ e portanto:

$$(I-A)^2=I-2A+A$$

$$(I-A)^2=I-A$$
Logo, $I-A$ é idempotente.

# Questão 5

__Seja $x_1, x_2, . . . , x_n$ uma amostra aleatória de uma distribuição conjunta com vetor de médias $\mu$ e matriz de variância-covariância $\Sigma$. Mostrar que__

$$E(\bar{X})=\mu$$
E

$$Cov(\bar{X})=\frac{1}{n}\Sigma$$

__Nota: Esta demonstração é apresentada por Johnson e Wichern na página 121.__

Primeiro, para $E(\bar{X})$, temos:

$$E(\bar{X})=E(\frac{\sum_{i=1}^nXi}{n})$$

$$E(\bar{X})=\frac{1}{n}\sum_{i=1}^nE(Xi)$$
$$E(\bar{X})=\frac{1}{n}\sum_{i=1}^n \mu$$

$$E(\bar{X})=\frac{1}{n}n \mu$$

$$E(\bar{X})=\mu$$

Já para $Cov(\bar{X})$:

$$Cov(\bar{X})=E[(\bar{X}-E\bar{X})(\bar{X}-E\bar{X})^T]$$

$$Cov(\bar{X})=E[(\bar{X}-\mu)(\bar{X}-\mu)^T]$$

$$Cov(\bar{X})=E[\frac{\sum_{j=1}^n(X_j-\mu)}{n}\frac{\sum_{l=1}^n(X_l-\mu)^T}{n}]$$

$$Cov(\bar{X})=E[\frac{\sum_{j=1}^n(X_j-\mu)}{n}\frac{\sum_{l=1}^n(X_l-\mu)^T}{n}]$$

$$Cov(\bar{X})=E[\frac{1}{n^2}\sum_{j=1}^n\sum_{l=1}^n(X_j-\mu)(X_l-\mu)^T]$$

$$Cov(\bar{X})=\frac{1}{n^2}\sum_{j=1}^n\sum_{l=1}^nE[(X_j-\mu)(X_l-\mu)^T]$$
Obs.: Não entendi o porquê do próximo passo, mas aparentemente os elementos do somatório são nulos para $j\neq l$.

$$Cov(\bar{X})=\frac{1}{n^2}\sum_{j=1}^nE[(X_j-\mu)(X_j-\mu)^T]$$

$$Cov(\bar{X})=\frac{1}{n^2}\sum_{j=1}^n \Sigma$$
$$Cov(\bar{X})=\frac{1}{n^2}n \Sigma$$

$$Cov(\bar{X})=\frac{1}{n} \Sigma$$

# Questão 6

__Utilizando o R verifique, através de exemplos, que uma matriz de projeção tem autovalores somente no conjunto $\{0, 1\}$. A demonstração pode ser feita utilizando a equação característica e lembrando que se $M$ é uma matriz de projeção, então $M = M^2 = M^T$.__

Igual a questão 3.

# Questão 7

__Seja $X$ uma matriz de dados $(n × p)$ com matriz de covariância $S$. Sejam $\lambda_1, . . . , \lambda_p$ os autovalores de $S$.__

__(a) Mostre que a soma das variâncias $s_{ii}$ de $X$ (variação amostral total) é dada por $\lambda_1 + . . . + \lambda_p$.__

DÚVIDA.

__(b) Mostre que a variância amostral generalizada é dada por $\lambda_1 × . . . × \lambda_p.__

DÚVIDA.

__(c) Mostre que a variância amostral generalizada se anula se as colunas de $X$ somarem zero.__

DÚVIDA.

# Questão 8

__Seja $A$ uma matriz quadrada $(k × k)$ positiva definida. Mostre que,__

__(a) $(A^{1/2})^T = A^{1/2}$.__

$$(A^{1/2})^T=(A^T)^{1/2}=A^{1/2}$$

Na última igualidade, foi usado que $A=A^T$, pois se $A$ é positiva definida, é também simétrica.

__(b) $A^{1/2}A^{1/2}=A$.__

DÚVIDA.

__(c) $(A^{1/2})^{-1}=CD^{-1/2}C^T$, sendo $D^{-1/2}=diag(1/\sqrt{\lambda_1},...,1/\sqrt{\lambda_k})$.__

DÚVIDA: não definiu $C$.

__(d) $A^{1/2}A^{-1/2}=A^{-1/2}A^{1/2}=I$.__

DÚVIDA. Provar que $A^{1/2}$ é não singular.

__(e) $A^{-1/2}A^{-1/2}=A^{-1}$.__

DÚVIDA.

# Questão 9

__Considere uma matriz de correlação $(r×r)$ com a mesma correlação ($\rho$) em todas as células fora da diagonal. Encontre os autovalores e autovetores desta matriz quando $r = 2, 3, 4$. Generalize seus resultados para qualquer número $r$ de variáveis. Como exemplo, faça $\rho = 0.1, 0.3, 0.5, 0.7, 0.9$.__

Casos $r=2$:

```{r}
R <- matrix(c(1, 0.1, 1, 0.1), 2)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.3, 1, 0.3), 2)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.5, 1, 0.5), 2)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.7, 1, 0.7), 2)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.9, 1, 0.9), 2)
eigen(R)
```

Por algum motivo, o segundo autovalor está sendo truncado na saída. Na realidade, ele assume os seguintes valores: 0.9, 0.7, 0.5, 0.3, 0.1, para $\rho=0.1, 0.3, 0.5, 0.7, 0.9$ respectivamente.

Casos $r=3$:

```{r}
R <- matrix(c(1, 0.1, 0.1, 0.1, 1, 0.1, 0.1, 0.1, 1), 3)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.3, 0.3, 0.3, 1, 0.3, 0.3, 0.3, 1), 3)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 1), 3)
eigen(R)
```


```{r}
R <- matrix(c(1, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 1), 3)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.9, 0.9, 0.9, 1, 0.9, 0.9, 0.9, 1), 3)
eigen(R)
```

Por fim, casos $r=4$:

```{r}
R <- matrix(c(1, 0.1, 0.1, 0.1, 0.1, 1, 0.1, 0.1, 0.1, 0.1, 1, 0.1, 0.1, 0.1, 0.1, 1), 4)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.3, 0.3, 0.3, 0.3, 1, 0.3, 0.3, 0.3, 0.3, 1, 0.3, 0.3, 0.3, 0.3, 1), 4)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.5, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 1), 4)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 1), 4)
eigen(R)
```

```{r}
R <- matrix(c(1, 0.9, 0.9, 0.9, 0.9, 1, 0.9, 0.9, 0.9, 0.9, 1, 0.9, 0.9, 0.9, 0.9, 1), 4)
eigen(R)
```

Observando o padrão que se repete, percebe-se que os autovalores $\lambda_2, ..., \lambda_r$ são sempre iguais a $1-\rho$, resultando em $\sum_{i=2}^r \lambda_i=\sum_{i=2}^r(1-\rho)=(1-r)(1-\rho)$. Além disso, sabemos que $\sum_{i=1}^r \lambda_i=r$ e, portanto, $\lambda_1+\sum_{i=2}^r \lambda_i=r$, resultando em $\lambda_1=r-\sum_{i=2}^r \lambda_i$. Daí, conclui-se que o primeiro (e maior) autovalor $\lambda_1$ é dado por:

$$\lambda_1=r-\sum_{i=2}^r \lambda_i=r-(r-1)(1-\rho)=r-(r-r\rho-1+\rho)=r\rho+1-\rho=1+\rho(r-1)$$

# Questão 10

__Considere a decomposição espectral de uma matrix $A_{p×p}$ positiva definida, isto é, $A_{p×p} = B \Delta B^T$. Seja__

$$A=\left(\begin{array}{cccc}
3 & 2 & 3 & 2 \\
2 & 5 & 1 & 1 \\
3 & 1 & 8 & 2 \\
2 & 1 & 2 & 3 
\end{array}\right)$$

__(a) Obtenha $B$ e $\Delta$.__

```{r}
A <- matrix(c(3, 2, 3, 2, 2, 5, 1, 1, 3, 1, 8, 2, 2, 1, 2, 3), 4)
eigen(A)
```

Logo:

$$B=\left(\begin{array}{cccc}
-0.4469286 & 0.17735073 & 0.2628243 & 0.8364956 \\
-0.3274936 & 0.83843658 & -0.3647871 & -0.2381228 \\
-0.7602805 & -0.50687972 & -0.3616326 & -0.1851172 \\
-0.3390817 & 0.09297191 & 0.8167470 & -0.4574978 
\end{array}\right)$$

E

$$\Delta=\left(\begin{array}{cccc}
11.0862856 & 0 & 0 & 0 \\
0 & 4.9293848 & 0 & 0 \\
0 & 0 & 2.3114101 & 0 \\
0 & 0 & 0 &  0.6729195 
\end{array}\right)$$

__(b) Obtenha $A^{1/2}$ e mostre que $(A^{1/2})^2=A$.__

$A^{1/2}$ é dado por:

```{r}
A^0.5
```

Além disso, temos que $(A^{1/2})^2$ é dado por:

```{r}
(A^0.5)^2
```

que é igual a $A$.

__(c) Obtenha $(A^{1/2})^{-1}$ (descreva seus elementos e mostre qe $(A^{1/2})^{-1}A^{1/2}=I$.__

$(A^{1/2})^{-1}$ é dado por:

```{r}
solve(A^0.5)
```

E $(A^{1/2})^{-1}A^{1/2}$:

```{r}
round(solve(A^0.5)%*%(A^0.5))
```

Que é igual à matriz identidade ($I$).

__Para as questões 11 e 12, considere o seguinte enunciado e marque todos os itens corretos.__

__Considere a decomposição espectral de uma matriz $A$ como__

$$A=PDP^T$$

__sendo,__

$$P=\left(\begin{array}{cccc}
-0.455 & 0.580 & 0.675 \\
-0.846 & -0.045 & -0.531 \\
-0.278 & -0.813 & 0.511
\end{array}\right)$$

__e $D=diag(13.542, 3.935,-2, 477)$.__

# Questão 11

__Marque Verdadeiro (V) ou Falso (F).__

__Sobre a decomposição espectral de $A^2$:__

__(a) É igual a $P^2D^2(P^T)^2$.__

```{r}
P <- matrix(c(-0.455, 0.580, 0.675, -0.846, -0.045, -0.531, -0.278, -0.813, 0.511), 3, byrow=T)
D <- diag(c(13.542, 3.935, -2.477))
A <- P%*%D%*%t(P)
```

```{r}
A%*%A
eigen(A%*%A)
```

A matriz de autovetores de $A^2$ é igual à matriz $P$.

Olhando primeiro para $D^2$:

```{r}
D%*%D
```

É equivalente à matriz diagonal de autovalores de $A^2$.

Agora, olhando para $P^2$:

```{r}
P%*%P
```

Não é equivalente à matriz de autovetores de $A^2$.

Falso.

__(b) É igual a $PD^2P^T$.__

Verdadeiro, já observamos que $D^2$ é igual à matriz diagonal de autovalores de $A^2$ e que $P$ é igual à matriz de autovetores de 4A^2$.


__(c) É igual a $(PDP^T)(PDP^T)$.__

```{r}
D%*%t(P)%*%P%*%D
```
Ou seja, temos que $DP^TPD \approx D^2$, satisfazendo também a estrutura da decomposição espectral de $A^2$.

Verdadeiro.

__(d) É igual a $P^2D(P^T)^2$.__

Falso. Já vimos que $P^2$ não equivale à matriz de autovetores de $A^2$ e também sabemos que a matriz diagonal de autovalores de $A^2$ é igual a $D^2$, que é diferente de $D$.

__(e) Não pode ser determinada.__

Falso. Já determinamos.

__(f) $P$ é a matriz formada pelos autovetores de $A$.__

Verdadeiro, isso deriva da definição de decomposição espectral.

# Questão 12

__Marque o item incorreto. Considere $\lambda_i$ o i-ésimo autovalor de $A$.__

__(a) $tr(A)=\sum_{i=1}^3 \lambda_i=tr(D)$.__

```{r}
sum(diag(A))
sum(eigen(A)$values)
sum(diag(D))
```

Verdadeiro

__(b) $A^{-1}=P^{-1}D^{-1}(P^T)^{-1}$.__

```{r}
solve(A)
solve(P)%*%solve(D)%*%solve(t(P))
```

Falso.

__(c) $A=APP^T$.__

```{r}
A
A%*%P%*%t(P)
```

Verdadeiro.

__(d) $P^TAP=D$.__

```{r}
t(P)%*%A%*%P
D
```

Verdadeiro.

__(e) $|A|=\prod_{i=1}^3 \lambda_i$.__

```{r}
det(A)
prod(eigen(A)$values)
```

Verdadeiro.

